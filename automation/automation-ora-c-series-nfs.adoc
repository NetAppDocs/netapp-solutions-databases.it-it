---
sidebar: sidebar 
permalink: automation/automation-ora-c-series-nfs.html 
keywords: Database, Oracle, Azure, ANF, Ansible, Automation 
summary: Questa soluzione fornisce una panoramica e dettagli per la distribuzione automatizzata di Oracle in NetApp AFF C-Series come storage di database primario con protocollo NFS.  Il database Oracle viene distribuito come database contenitore con dNFS abilitato. 
---
= TR-4992: Distribuzione Oracle semplificata e automatizzata su NetApp C-Series con NFS
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


Allen Cao, Niyaz Mohamed, NetApp

[role="lead"]
Questa soluzione fornisce una panoramica e dettagli per la distribuzione automatizzata di Oracle in NetApp AFF C-Series come storage di database primario con protocollo NFS.  Il database Oracle viene distribuito come database contenitore con dNFS abilitato.



== Scopo

NetApp AFF C-Series è un sistema di storage flash ad alta capacità che rende l'all-flash più accessibile e conveniente per lo storage unificato.  Offre prestazioni sufficienti per molti carichi di lavoro di database Oracle di livello 1 o 2.  Grazie al software di gestione dati NetApp ONTAP , i sistemi AFF C-Series offrono efficienza leader del settore, flessibilità superiore, servizi dati di prima classe e integrazione cloud per aiutarti a scalare la tua infrastruttura IT, semplificare la gestione dei dati e ridurre i costi di storage e il consumo energetico.

Questa documentazione illustra la distribuzione semplificata dei database Oracle in NetApp C-Series tramite montaggi NFS utilizzando l'automazione Ansible.  Il database Oracle viene distribuito in una configurazione di database contenitore (CDB) e database collegabili (PDB) con protocollo Oracle dNFS abilitato per aumentare le prestazioni.  Inoltre, la soluzione fornisce le migliori pratiche per la configurazione di reti di archiviazione e macchine virtuali di archiviazione (SVM) con protocollo NFS sui controller di archiviazione della serie C.  La soluzione include anche informazioni sul backup, il ripristino e la clonazione rapidi del database Oracle con lo strumento NetApp SnapCenter UI.

Questa soluzione affronta i seguenti casi d'uso:

* Distribuzione automatizzata del database contenitore Oracle sui controller di storage NetApp C-Series.
* Protezione e clonazione del database Oracle su C-Series con lo strumento SnapCenter UI.




== Pubblico

Questa soluzione è destinata alle seguenti persone:

* Un DBA che vorrebbe implementare Oracle su NetApp C-Series.
* Un architetto di soluzioni di database che vorrebbe testare i carichi di lavoro Oracle su NetApp C-Series.
* Un amministratore di storage che desidera distribuire e gestire un database Oracle su NetApp C-Series.
* Un proprietario di un'applicazione che vorrebbe installare un database Oracle su NetApp C-Series.




== Ambiente di test e convalida della soluzione

I test e la convalida di questa soluzione sono stati eseguiti in un ambiente di laboratorio che potrebbe non corrispondere all'ambiente di distribuzione finale.  Vedi la sezione<<Fattori chiave per la considerazione dell'implementazione>> per maggiori informazioni.



=== Architettura

image:automation-ora-c-series-nfs-architecture.png["Questa immagine fornisce un quadro dettagliato della configurazione di distribuzione di Oracle nel cloud pubblico AWS con iSCSI e ASM."]



=== Componenti hardware e software

[cols="33%, 33%, 33%"]
|===


3+| *Hardware* 


| NetApp C-Series C400 | ONTAP versione 9.13.1P3 | Due ripiani per dischi / 24 dischi con capacità di 278 TiB 


| VM per server DB | 4 vCPU, 16 GiB di RAM | Due istanze di VM Linux per la distribuzione simultanea 


| VM per SnapCenter | 4 vCPU, 16 GiB di RAM | Un'istanza di macchina virtuale Windows 


3+| *Software* 


| RedHat Linux | RHEL Linux 8.6 (LVM) - x64 Gen2 | Abbonamento RedHat distribuito per i test 


| Server Windows | DataCenter x64 Gen2 del 2022 | Hosting del server SnapCenter 


| Database Oracle | Versione 19.18 | Patch RU applicata p34765931_190000_Linux-x86-64.zip 


| Oracle OPatch | Versione 12.2.0.1.36 | Ultima patch p6880880_190000_Linux-x86-64.zip 


| Server SnapCenter | Versione 5.0 | Distribuzione del gruppo di lavoro 


| Apri JDK | Versione java-11-openjdk | Requisiti del plugin SnapCenter sulle VM DB 


| NFS | Versione 3.0 | Oracle dNFS abilitato 


| Ansible | nucleo 2.16.2 | Python 3.6.8 
|===


=== Configurazione del database Oracle nell'ambiente di laboratorio

[cols="33%, 33%, 33%"]
|===


3+|  


| *Server* | *Banca dati* | *Archiviazione DB* 


| ora_01 | NTAP1(NTAP1_PDB1,NTAP1_PDB2,NTAP1_PDB3) | /u01, /u02, /u03 Montaggi NFS su volumi C400 


| ora_02 | NTAP2(NTAP2_PDB1,NTAP2_PDB2,NTAP2_PDB3) | /u01, /u02, /u03 Montaggi NFS su volumi C400 
|===


=== Fattori chiave per la considerazione dell'implementazione

* *Layout di archiviazione del database Oracle.*  In questa distribuzione Oracle automatizzata, per impostazione predefinita forniamo tre volumi di database per ogni database per ospitare file binari, dati e log di Oracle.  I volumi vengono montati sul server Oracle DB come /u01 - binario, /u02 - dati, /u03 - log tramite NFS.  Per ridondanza, i file di controllo doppi sono configurati sui punti di montaggio /u02 e /u03.
* *Implementazione di più server DB.*  La soluzione di automazione può distribuire un database contenitore Oracle su più server DB in un'unica esecuzione del playbook Ansible.  Indipendentemente dal numero di server DB, l'esecuzione del playbook rimane la stessa.  È possibile distribuire più database contenitore su una singola istanza VM ripetendo la distribuzione con ID istanza database diversi (Oracle SID).  Ma assicurati che ci sia memoria sufficiente sull'host per supportare i database distribuiti.
* *configurazione dNFS.*  Utilizzando dNFS (disponibile a partire da Oracle 11g), un database Oracle in esecuzione su una DB VM può gestire un I/O significativamente maggiore rispetto al client NFS nativo.  Per impostazione predefinita, la distribuzione automatizzata di Oracle configura dNFS su NFSv3.
* *Bilanciamento del carico sulla coppia di controller C400.*  Disporre i volumi del database Oracle sui nodi del controller C400 in modo uniforme per bilanciare il carico di lavoro.  DB1 sul controller 1, DB2 sul controller 2 e così via.  Montare i volumi DB sul relativo indirizzo LIF locale.
* *Backup del database.*  NetApp fornisce una suite SnapCenter software per il backup, il ripristino e la clonazione del database con un'interfaccia utente intuitiva.  NetApp consiglia di implementare tale strumento di gestione per ottenere un backup snapshot rapido (in meno di un minuto), un ripristino rapido del database (in pochi minuti) e una clonazione del database.




== Distribuzione della soluzione

Le sezioni seguenti forniscono procedure dettagliate per la distribuzione automatizzata di Oracle 19c e informazioni sulla protezione e la clonazione del database Oracle dopo la distribuzione.



=== Prerequisiti per la distribuzione

[%collapsible%open]
====
Per la distribuzione sono richiesti i seguenti prerequisiti.

. Una coppia di controller di storage NetApp C-Series viene montata su rack, impilata e dotata dell'ultima versione del sistema operativo ONTAP installata e configurata.  Se necessario, fare riferimento a questa guida di installazione: https://docs.netapp.com/us-en/ontap-systems/c400/install-detailed-guide.html#step-1-prepare-for-installation["Guida dettagliata - AFF C400"^]
. Fornire due VM Linux come server Oracle DB.  Per i dettagli sulla configurazione dell'ambiente, vedere il diagramma dell'architettura nella sezione precedente.
. Fornire un server Windows per eseguire lo strumento NetApp SnapCenter UI con la versione più recente.  Per maggiori dettagli fare riferimento al seguente link:link:https://docs.netapp.com/us-en/snapcenter/install/task_install_the_snapcenter_server_using_the_install_wizard.html["Installare il server SnapCenter"^]
. Fornire una VM Linux come nodo controller Ansible con installata l'ultima versione di Ansible e Git.  Per maggiori dettagli fare riferimento al seguente link:link:https://docs.netapp.com/us-en/netapp-solutions-dataops/automation/getting-started.html["Introduzione all'automazione delle soluzioni NetApp ^"^] nella sezione -
`Setup the Ansible Control Node for CLI deployments on RHEL / CentOS` O
`Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian` .
+
Abilita l'autenticazione tramite chiave pubblica/privata SSH tra il controller Ansible e le VM del database.

. Dalla directory home dell'utente amministratore del controller Ansible, clonare una copia del toolkit di automazione della distribuzione NetApp Oracle per NFS.
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_oracle_deploy_nfs.git
----
. Fase successiva ai file di installazione di Oracle 19c nella directory DB VM /tmp/archive con autorizzazione 777.
+
....
installer_archives:
  - "LINUX.X64_193000_db_home.zip"
  - "p34765931_190000_Linux-x86-64.zip"
  - "p6880880_190000_Linux-x86-64.zip"
....


====


=== Configurazione di rete e SVM su C-Series per Oracle

[%collapsible%open]
====
Questa sezione della guida alla distribuzione illustra le best practice per configurare la macchina virtuale di rete e storage (SVM) sul controller C-Series per carichi di lavoro Oracle con protocollo NFS utilizzando l'interfaccia utente di ONTAP System Manager.

. Accedere a ONTAP System Manager per verificare che, dopo l'installazione iniziale del cluster ONTAP , i domini broadcast siano stati configurati con le porte Ethernet assegnate correttamente a ciascun dominio.  In genere, dovrebbe esserci un dominio broadcast per il cluster, un dominio broadcast per la gestione e un dominio broadcast per il carico di lavoro, ad esempio i dati.
+
image:automation-ora-c-series-nfs-net-001.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Da RETE - Porte Ethernet, fare clic su `Link Aggregate Group` per creare una porta a0a del gruppo di aggregazione dei collegamenti LACP, che fornisce bilanciamento del carico e failover tra le porte membro nella porta del gruppo di aggregazione.  Sui controller C400 sono disponibili 4 porte dati: e0e, e0f, e0g, e0h.
+
image:automation-ora-c-series-nfs-net-002.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Selezionare le porte Ethernet nel gruppo, `LACP` per la modalità e `Port` per la distribuzione del carico.
+
image:automation-ora-c-series-nfs-net-003.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Convalida la porta LACP a0a creata e il dominio di trasmissione `Data` ora è operativo sulla porta LACP.
+
image:automation-ora-c-series-nfs-net-004.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"] image:automation-ora-c-series-nfs-net-005.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Da `Ethernet Ports` , clicca `VLAN` per aggiungere una VLAN su ciascun nodo controller per il carico di lavoro Oracle sul protocollo NFS.
+
image:automation-ora-c-series-nfs-net-006.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"] image:automation-ora-c-series-nfs-net-007.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"] image:automation-ora-c-series-nfs-net-008.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Accedere ai controller della serie C dall'IP di gestione del cluster tramite SSH per verificare che i gruppi di failover di rete siano configurati correttamente.  ONTAP crea e gestisce automaticamente i gruppi di failover.
+
....

HCG-NetApp-C400-E9U9::> net int failover-groups show
  (network interface failover-groups show)
                                  Failover
Vserver          Group            Targets
---------------- ---------------- --------------------------------------------
Cluster
                 Cluster
                                  HCG-NetApp-C400-E9U9a:e0c,
                                  HCG-NetApp-C400-E9U9a:e0d,
                                  HCG-NetApp-C400-E9U9b:e0c,
                                  HCG-NetApp-C400-E9U9b:e0d
HCG-NetApp-C400-E9U9
                 Data
                                  HCG-NetApp-C400-E9U9a:a0a,
                                  HCG-NetApp-C400-E9U9a:a0a-3277,
                                  HCG-NetApp-C400-E9U9b:a0a,
                                  HCG-NetApp-C400-E9U9b:a0a-3277
                 Mgmt
                                  HCG-NetApp-C400-E9U9a:e0M,
                                  HCG-NetApp-C400-E9U9b:e0M
3 entries were displayed.

....
. Da `STORAGE - Storage VMs` , fare clic su +Aggiungi per creare una SVM per Oracle.
+
image:automation-ora-c-series-nfs-svm-001.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Assegna un nome al tuo Oracle SVM, controlla `Enable NFS` E `Allow NFS client access` .
+
image:automation-ora-c-series-nfs-svm-002.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Aggiungi criterio di esportazione NFS `Default` regole.
+
image:automation-ora-c-series-nfs-svm-003.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. In `NETWORK INTERFACE` , compila l'indirizzo IP su ciascun nodo per gli indirizzi NFS lif.
+
image:automation-ora-c-series-nfs-svm-004.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Verificare che SVM per Oracle sia attivo/in esecuzione e che lo stato NFS lifs sia attivo.
+
image:automation-ora-c-series-nfs-svm-005.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"] image:automation-ora-c-series-nfs-svm-006.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Da `STORAGE-Volumes` scheda per aggiungere volumi NFS per il database Oracle.
+
image:automation-ora-c-series-nfs-vol-001.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Assegna un nome al volume, assegna la capacità e il livello di prestazioni.
+
image:automation-ora-c-series-nfs-vol-002.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. In `Access Permission` , seleziona la policy predefinita creata nel passaggio precedente.  Deseleziona `Enable Snapshot Copies` poiché preferiamo utilizzare SnapCenter per creare snapshot coerenti con l'applicazione.
+
image:automation-ora-c-series-nfs-vol-003.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

. Creare tre volumi DB per ciascun server DB: server_name_u01 - binario, server_name_u02 - dati, server_name_u03 - registri.
+
image:automation-ora-c-series-nfs-vol-004.png["Questa immagine fornisce uno screenshot per la configurazione del controller della serie C"]

+

NOTE: Per garantire il corretto funzionamento dell'automazione, la convenzione di denominazione del volume DB deve seguire rigorosamente il formato indicato sopra.



Questo completa la configurazione del controller della serie C per Oracle.

====


=== File dei parametri di automazione

[%collapsible%open]
====
Il playbook Ansible esegue attività di installazione e configurazione del database con parametri predefiniti.  Per questa soluzione di automazione Oracle, sono presenti tre file di parametri definiti dall'utente che necessitano dell'input dell'utente prima dell'esecuzione del playbook.

* host: definiscono i target su cui viene eseguito il playbook di automazione.
* vars/vars.yml: il file delle variabili globali che definisce le variabili che si applicano a tutti i target.
* host_vars/host_name.yml: il file delle variabili locali che definisce le variabili che si applicano solo a una destinazione denominata.  Nel nostro caso d'uso, si tratta dei server Oracle DB.


Oltre a questi file di variabili definiti dall'utente, esistono diversi file di variabili predefiniti che contengono parametri predefiniti che non richiedono modifiche, a meno che non siano strettamente necessari.  Le sezioni seguenti mostrano come configurare i file delle variabili definite dall'utente.

====


=== Configurazione dei file dei parametri

[%collapsible%open]
====
. Obiettivo Ansible `hosts` configurazione dei file:
+
[source, shell]
----
# Enter Oracle servers names to be deployed one by one, follow by each Oracle server public IP address, and ssh private key of admin user for the server.
[oracle]
ora_01 ansible_host=10.61.180.21 ansible_ssh_private_key_file=ora_01.pem
ora_02 ansible_host=10.61.180.23 ansible_ssh_private_key_file=ora_02.pem

----


. Globale `vars/vars.yml` configurazione dei file
+
[source, shell]
----
######################################################################
###### Oracle 19c deployment user configuration variables       ######
###### Consolidate all variables from ONTAP, linux and oracle   ######
######################################################################

###########################################
### ONTAP env specific config variables ###
###########################################

# Prerequisite to create three volumes in NetApp ONTAP storage from System Manager or cloud dashboard with following naming convention:
# db_hostname_u01 - Oracle binary
# db_hostname_u02 - Oracle data
# db_hostname_u03 - Oracle redo
# It is important to strictly follow the name convention or the automation will fail.


###########################################
### Linux env specific config variables ###
###########################################

redhat_sub_username: XXXXXXXX
redhat_sub_password: XXXXXXXX


####################################################
### DB env specific install and config variables ###
####################################################

# Database domain name
db_domain: solutions.netapp.com

# Set initial password for all required Oracle passwords. Change them after installation.
initial_pwd_all: XXXXXXXX

----


. Server DB locale `host_vars/host_name.yml` configurazione come ora_01.yml, ora_02.yml ...
+
[source, shell]
----
# User configurable Oracle host specific parameters

# Enter container database SID. By default, a container DB is created with 3 PDBs within the CDB
oracle_sid: NTAP1

# Enter database shared memory size or SGA. CDB is created with SGA at 75% of memory_limit, MB. The grand total of SGA should not exceed 75% available RAM on node.
memory_limit: 8192

# Local NFS lif ip address to access database volumes
nfs_lif: 172.30.136.68

----


====


=== Esecuzione del playbook

[%collapsible%open]
====
Il toolkit di automazione contiene in totale cinque playbook.  Ognuno di essi esegue blocchi di attività diversi e ha scopi diversi.

....
0-all_playbook.yml - execute playbooks from 1-4 in one playbook run.
1-ansible_requirements.yml - set up Ansible controller with required libs and collections.
2-linux_config.yml - execute Linux kernel configuration on Oracle DB servers.
4-oracle_config.yml - install and configure Oracle on DB servers and create a container database.
5-destroy.yml - optional to undo the environment to dismantle all.
....
Esistono tre opzioni per eseguire i playbook con i seguenti comandi.

. Eseguire tutti i playbook di distribuzione in un'unica esecuzione combinata.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml
----
. Eseguire i playbook uno alla volta con la sequenza numerica da 1 a 4.
+
[source, cli]]
----
ansible-playbook -i hosts 1-ansible_requirements.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 2-linux_config.yml -u admin -e @vars/vars.yml
----
+
[source, cli]
----
ansible-playbook -i hosts 4-oracle_config.yml -u admin -e @vars/vars.yml
----
. Eseguire 0-all_playbook.yml con un tag.
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t ansible_requirements
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t linux_config
----
+
[source, cli]
----
ansible-playbook -i hosts 0-all_playbook.yml -u admin -e @vars/vars.yml -t oracle_config
----
. Annulla l'ambiente
+
[source, cli]
----
ansible-playbook -i hosts 5-destroy.yml -u admin -e @vars/vars.yml
----


====


=== Convalida post-esecuzione

[%collapsible%open]
====
Dopo l'esecuzione del playbook, accedi alla VM del server Oracle DB per verificare che Oracle sia installato e configurato e che un database contenitore sia stato creato correttamente.  Di seguito è riportato un esempio di convalida del database Oracle su DB VM ora_01 o ora_02.

. Convalida i mount NFS
+
....

[admin@ora_01 ~]$ cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Wed Oct 18 19:43:31 2023
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
/dev/mapper/rhel-root   /                       xfs     defaults        0 0
UUID=aff942c4-b224-4b62-807d-6a5c22f7b623 /boot                   xfs     defaults        0 0
/dev/mapper/rhel-swap   none                    swap    defaults        0 0
/root/swapfile swap swap defaults 0 0
172.21.21.100:/ora_01_u01 /u01 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
172.21.21.100:/ora_01_u02 /u02 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
172.21.21.100:/ora_01_u03 /u03 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0


[admin@ora_01 tmp]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G     0  7.8G   0% /dev/shm
tmpfs                      7.8G   18M  7.8G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rhel-root       44G   28G   17G  62% /
/dev/sda1                 1014M  258M  757M  26% /boot
tmpfs                      1.6G   12K  1.6G   1% /run/user/42
tmpfs                      1.6G  4.0K  1.6G   1% /run/user/1000
172.21.21.100:/ora_01_u01   50G  8.7G   42G  18% /u01
172.21.21.100:/ora_01_u02  200G  384K  200G   1% /u02
172.21.21.100:/ora_01_u03  100G  320K  100G   1% /u03

[admin@ora_02 ~]$ df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G     0  7.8G   0% /dev/shm
tmpfs                      7.8G   18M  7.8G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/rhel-root       44G   28G   17G  63% /
/dev/sda1                 1014M  258M  757M  26% /boot
tmpfs                      1.6G   12K  1.6G   1% /run/user/42
tmpfs                      1.6G  4.0K  1.6G   1% /run/user/1000
172.21.21.101:/ora_02_u01   50G  7.8G   43G  16% /u01
172.21.21.101:/ora_02_u02  200G  320K  200G   1% /u02
172.21.21.101:/ora_02_u03  100G  320K  100G   1% /u03

....
. Convalida l'ascoltatore Oracle
+
....

[admin@ora_02 ~]$ sudo su
[root@ora_02 admin]# su - oracle
[oracle@ora_02 ~]$ lsnrctl status listener.ntap2

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 29-MAY-2024 12:13:30

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=ora_02.cie.netapp.com)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     LISTENER.NTAP2
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                23-MAY-2024 16:13:03
Uptime                    5 days 20 hr. 0 min. 26 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP2/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ora_02/listener.ntap2/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ora_02.cie.netapp.com)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ora_02.cie.netapp.com)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP2/admin/NTAP2/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "192551f1d7e65fc3e06308b43d0a63ae.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "1925529a43396002e06308b43d0a2d5a.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "1925530776b76049e06308b43d0a49c3.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "NTAP2.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "NTAP2XDB.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb1.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb2.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
Service "ntap2_pdb3.solutions.netapp.com" has 1 instance(s).
  Instance "NTAP2", status READY, has 1 handler(s) for this service...
The command completed successfully
[oracle@ora_02 ~]$

....
. Convalida del database Oracle e dNFS
+
....

[oracle@ora-01 ~]$ cat /etc/oratab
#
# This file is used by ORACLE utilities.  It is created by root.sh
# and updated by either Database Configuration Assistant while creating
# a database or ASM Configuration Assistant while creating ASM instance.

# A colon, ':', is used as the field terminator.  A new line terminates
# the entry.  Lines beginning with a pound sign, '#', are comments.
#
# Entries are of the form:
#   $ORACLE_SID:$ORACLE_HOME:<N|Y>:
#
# The first and second fields are the system identifier and home
# directory of the database respectively.  The third field indicates
# to the dbstart utility that the database should , "Y", or should not,
# "N", be brought up at system boot time.
#
# Multiple entries with the same $ORACLE_SID are not allowed.
#
#
NTAP1:/u01/app/oracle/product/19.0.0/NTAP1:Y


[oracle@ora-01 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Feb 1 16:37:51 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode, log_mode from v$database;

NAME      OPEN_MODE            LOG_MODE
--------- -------------------- ------------
NTAP1     READ WRITE           ARCHIVELOG

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 NTAP1_PDB1                     READ WRITE NO
         4 NTAP1_PDB2                     READ WRITE NO
         5 NTAP1_PDB3                     READ WRITE NO
SQL> select name from v$datafile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/system01.dbf
/u02/oradata/NTAP1/sysaux01.dbf
/u02/oradata/NTAP1/undotbs01.dbf
/u02/oradata/NTAP1/pdbseed/system01.dbf
/u02/oradata/NTAP1/pdbseed/sysaux01.dbf
/u02/oradata/NTAP1/users01.dbf
/u02/oradata/NTAP1/pdbseed/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb1/users01.dbf

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/NTAP1_pdb2/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb2/users01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/system01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/sysaux01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/undotbs01.dbf
/u02/oradata/NTAP1/NTAP1_pdb3/users01.dbf

19 rows selected.

SQL> select name from v$controlfile;

NAME
--------------------------------------------------------------------------------
/u02/oradata/NTAP1/control01.ctl
/u03/orareco/NTAP1/control02.ctl

SQL> select member from v$logfile;

MEMBER
--------------------------------------------------------------------------------
/u03/orareco/NTAP1/onlinelog/redo03.log
/u03/orareco/NTAP1/onlinelog/redo02.log
/u03/orareco/NTAP1/onlinelog/redo01.log

SQL> select svrname, dirname from v$dnfs_servers;

SVRNAME
--------------------------------------------------------------------------------
DIRNAME
--------------------------------------------------------------------------------
172.21.21.100
/ora_01_u02

172.21.21.100
/ora_01_u03

172.21.21.100
/ora_01_u01


....
. Accedi a Oracle Enterprise Manager Express per convalidare il database.
+
image:automation-ora-c-series-nfs-em-001.png["Questa immagine fornisce la schermata di accesso per Oracle Enterprise Manager Express"] image:automation-ora-c-series-nfs-em-002.png["Questa immagine fornisce la vista del database del contenitore da Oracle Enterprise Manager Express"] image:automation-ora-c-series-nfs-em-003.png["Questa immagine fornisce la vista del database del contenitore da Oracle Enterprise Manager Express"]



====


=== Backup, ripristino e clonazione di Oracle con SnapCenter

[%collapsible%open]
====
NetApp consiglia lo strumento SnapCenter UI per gestire il database Oracle distribuito in C-Series.  Fare riferimento a TR-4979link:../oracle/aws-ora-fsx-vmc-guestmount.html#oracle-backup-restore-and-clone-with-snapcenter["Oracle semplificato e autogestito in VMware Cloud su AWS con FSx ONTAP montato su guest"^] sezione `Oracle backup, restore, and clone with SnapCenter` per i dettagli sulla configurazione SnapCenter e sull'esecuzione dei flussi di lavoro di backup, ripristino e clonazione del database.

====


== Dove trovare ulteriori informazioni

Per saperne di più sulle informazioni descritte nel presente documento, consultare i seguenti documenti e/o siti web:

* link:https://www.netapp.com/pdf.html?item=/media/81583-da-4240-aff-c-series.pdf["NetApp AFF serie C"^]
* link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2["Distribuzione di Oracle Direct NFS"^]

